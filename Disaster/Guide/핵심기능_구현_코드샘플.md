# í•µì‹¬ ê¸°ëŠ¥ êµ¬í˜„ ì½”ë“œ ìƒ˜í”Œ

---

## ğŸ“‹ ëª©ì°¨
1. [AI Orchestrator Agent](#ai-orchestrator-agent)
2. [GNN ê¸°ë°˜ ì—ë„ˆì§€ ì˜ˆì¸¡](#gnn-ê¸°ë°˜-ì—ë„ˆì§€-ì˜ˆì¸¡)
3. [ì˜¨í†¨ë¡œì§€ ì„œë¹„ìŠ¤](#ì˜¨í†¨ë¡œì§€-ì„œë¹„ìŠ¤)
4. [IoT ë°ì´í„° ìˆ˜ì§‘](#iot-ë°ì´í„°-ìˆ˜ì§‘)
5. [ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§](#ì‹¤ì‹œê°„-ëª¨ë‹ˆí„°ë§)

---

## ğŸ¤– AI Orchestrator Agent

### 1. Base Agent í´ë˜ìŠ¤

```python
# backend/src/agents/base_agent.py
from abc import ABC, abstractmethod
from typing import Dict, Any, List
from langchain.llms import OpenAI
from langchain.chat_models import ChatAnthropic
from langchain.prompts import PromptTemplate
import logging

logger = logging.getLogger(__name__)

class BaseAgent(ABC):
    """AI ì—ì´ì „íŠ¸ ê¸°ë³¸ í´ë˜ìŠ¤"""
    
    def __init__(self, agent_name: str, llm_provider: str = "openai"):
        self.agent_name = agent_name
        self.llm = self._init_llm(llm_provider)
        self.memory = []
        logger.info(f"Initialized agent: {agent_name}")
    
    def _init_llm(self, provider: str):
        """LLM ì´ˆê¸°í™”"""
        if provider == "openai":
            return OpenAI(temperature=0.7, model="gpt-4")
        elif provider == "anthropic":
            return ChatAnthropic(model="claude-3-opus-20240229")
        else:
            raise ValueError(f"Unsupported LLM provider: {provider}")
    
    @abstractmethod
    async def analyze(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """ë°ì´í„° ë¶„ì„ (ê° ì—ì´ì „íŠ¸ê°€ êµ¬í˜„)"""
        pass
    
    def add_to_memory(self, item: Dict[str, Any]):
        """ë©”ëª¨ë¦¬ì— ì¶”ê°€"""
        self.memory.append(item)
        # ë©”ëª¨ë¦¬ í¬ê¸° ì œí•œ (ìµœê·¼ 50ê°œë§Œ ìœ ì§€)
        if len(self.memory) > 50:
            self.memory = self.memory[-50:]
    
    def get_context(self) -> str:
        """ì»¨í…ìŠ¤íŠ¸ ë°˜í™˜"""
        return "\n".join([str(m) for m in self.memory[-10:]])
```

### 2. Disaster Analyzer Agent

```python
# backend/src/agents/disaster_analyzer.py
from src.agents.base_agent import BaseAgent
from typing import Dict, Any
import logging

logger = logging.getLogger(__name__)

class DisasterAnalyzerAgent(BaseAgent):
    """ì¬ë‚œ ìƒí™© ë¶„ì„ ì—ì´ì „íŠ¸"""
    
    def __init__(self):
        super().__init__("DisasterAnalyzer")
        self.severity_threshold = {
            "earthquake": {"minor": 3.0, "moderate": 5.0, "severe": 7.0},
            "typhoon": {"minor": 60, "moderate": 100, "severe": 150},  # km/h
        }
    
    async def analyze(self, disaster_data: Dict[str, Any]) -> Dict[str, Any]:
        """ì¬ë‚œ ë°ì´í„° ë¶„ì„"""
        disaster_type = disaster_data.get("type")
        magnitude = disaster_data.get("magnitude", 0)
        location = disaster_data.get("location")
        
        # ì‹¬ê°ë„ í‰ê°€
        severity = self._evaluate_severity(disaster_type, magnitude)
        
        # LLMì„ ì‚¬ìš©í•œ ì˜í–¥ ë²”ìœ„ ë¶„ì„
        prompt = f"""
        ë‹¤ìŒ ì¬ë‚œ ì •ë³´ë¥¼ ë¶„ì„í•˜ê³  ì˜í–¥ì„ ë°›ì„ ì—ë„ˆì§€ ì¸í”„ë¼ë¥¼ ì˜ˆì¸¡í•˜ì„¸ìš”:
        
        ì¬ë‚œ ìœ í˜•: {disaster_type}
        ê·œëª¨: {magnitude}
        ìœ„ì¹˜: {location}
        ì‹¬ê°ë„: {severity}
        
        ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”:
        1. ì˜ˆìƒ ì˜í–¥ ë°˜ê²½ (km)
        2. ì˜í–¥ì„ ë°›ì„ ê°€ëŠ¥ì„±ì´ ìˆëŠ” ì—ë„ˆì§€ ì„¤ë¹„ ìœ í˜•
        3. ì˜ˆìƒ í”¼í•´ ì •ë„
        4. ê¸´ê¸‰ ëŒ€ì‘ ìš°ì„ ìˆœìœ„
        """
        
        analysis = await self.llm.apredict(prompt)
        
        result = {
            "agent": self.agent_name,
            "disaster_type": disaster_type,
            "severity": severity,
            "magnitude": magnitude,
            "llm_analysis": analysis,
            "affected_radius_km": self._estimate_radius(disaster_type, magnitude),
            "priority": self._calculate_priority(severity, location)
        }
        
        self.add_to_memory(result)
        logger.info(f"Disaster analysis completed: {disaster_type} - Severity: {severity}")
        
        return result
    
    def _evaluate_severity(self, disaster_type: str, magnitude: float) -> str:
        """ì‹¬ê°ë„ í‰ê°€"""
        thresholds = self.severity_threshold.get(disaster_type, {})
        
        if magnitude < thresholds.get("minor", 0):
            return "minor"
        elif magnitude < thresholds.get("moderate", 0):
            return "moderate"
        elif magnitude < thresholds.get("severe", 0):
            return "severe"
        else:
            return "critical"
    
    def _estimate_radius(self, disaster_type: str, magnitude: float) -> float:
        """ì˜í–¥ ë°˜ê²½ ì¶”ì •"""
        if disaster_type == "earthquake":
            # ê°„ë‹¨í•œ ê³µì‹: ë°˜ê²½ = magnitude^2 * 5
            return min(magnitude ** 2 * 5, 500)  # ìµœëŒ€ 500km
        elif disaster_type == "typhoon":
            return min(magnitude * 2, 300)
        return 50  # ê¸°ë³¸ê°’
    
    def _calculate_priority(self, severity: str, location: Dict) -> int:
        """ìš°ì„ ìˆœìœ„ ê³„ì‚° (1-10, 10ì´ ê°€ì¥ ë†’ìŒ)"""
        severity_score = {
            "minor": 3,
            "moderate": 6,
            "severe": 8,
            "critical": 10
        }
        return severity_score.get(severity, 5)
```

### 3. Energy Analyzer Agent

```python
# backend/src/agents/energy_analyzer.py
from src.agents.base_agent import BaseAgent
from typing import Dict, Any, List
import logging

logger = logging.getLogger(__name__)

class EnergyAnalyzerAgent(BaseAgent):
    """ì—ë„ˆì§€ ìˆ˜ê¸‰ ë¶„ì„ ì—ì´ì „íŠ¸"""
    
    def __init__(self):
        super().__init__("EnergyAnalyzer")
    
    async def analyze(self, energy_data: Dict[str, Any]) -> Dict[str, Any]:
        """ì—ë„ˆì§€ í˜„í™© ë¶„ì„"""
        assets = energy_data.get("assets", [])
        current_production = energy_data.get("total_production", 0)
        current_demand = energy_data.get("total_demand", 0)
        
        # ì—ë„ˆì§€ ë°¸ëŸ°ìŠ¤ ê³„ì‚°
        balance = current_production - current_demand
        balance_ratio = balance / current_demand if current_demand > 0 else 0
        
        # ì‰ì—¬/ë¶€ì¡± ì§€ì—­ ì‹ë³„
        surplus_assets = [a for a in assets if a.get("net_energy", 0) > 0]
        deficit_assets = [a for a in assets if a.get("net_energy", 0) < 0]
        
        # LLM ë¶„ì„
        prompt = f"""
        í˜„ì¬ ì—ë„ˆì§€ ìƒí™©ì„ ë¶„ì„í•˜ì„¸ìš”:
        
        ì´ ìƒì‚°ëŸ‰: {current_production} kW
        ì´ ìˆ˜ìš”: {current_demand} kW
        ì—ë„ˆì§€ ë°¸ëŸ°ìŠ¤: {balance} kW ({balance_ratio:.2%})
        
        ì‰ì—¬ ì—ë„ˆì§€ ë³´ìœ  ì§€ì—­: {len(surplus_assets)}ê°œ
        ì—ë„ˆì§€ ë¶€ì¡± ì§€ì—­: {len(deficit_assets)}ê°œ
        
        ë‹¤ìŒì„ ì œì•ˆí•˜ì„¸ìš”:
        1. ì—ë„ˆì§€ ì¬ë¶„ë°° ì „ëµ
        2. ìš°ì„  ì§€ì›ì´ í•„ìš”í•œ ì§€ì—­
        3. ìµœì  ì—ë„ˆì§€ íë¦„ ê²½ë¡œ
        """
        
        recommendations = await self.llm.apredict(prompt)
        
        result = {
            "agent": self.agent_name,
            "balance": balance,
            "balance_ratio": balance_ratio,
            "status": "surplus" if balance > 0 else "deficit",
            "surplus_assets": [a["id"] for a in surplus_assets],
            "deficit_assets": [a["id"] for a in deficit_assets],
            "recommendations": recommendations,
            "redistribution_needed": abs(balance) > current_demand * 0.1
        }
        
        self.add_to_memory(result)
        logger.info(f"Energy analysis completed: Balance: {balance:.2f} kW")
        
        return result
```

### 4. Decision Maker Agent

```python
# backend/src/agents/decision_maker.py
from src.agents.base_agent import BaseAgent
from typing import Dict, Any, List
import logging

logger = logging.getLogger(__name__)

class DecisionMakerAgent(BaseAgent):
    """ìµœì¢… ì˜ì‚¬ê²°ì • ì—ì´ì „íŠ¸"""
    
    def __init__(self):
        super().__init__("DecisionMaker")
    
    async def make_decision(
        self,
        disaster_analysis: Dict[str, Any],
        energy_analysis: Dict[str, Any],
        grid_analysis: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ì¢…í•© ë¶„ì„ í›„ ì˜ì‚¬ê²°ì •"""
        
        # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        context = f"""
        ì¬ë‚œ ë¶„ì„ ê²°ê³¼:
        - ì¬ë‚œ ìœ í˜•: {disaster_analysis.get('disaster_type')}
        - ì‹¬ê°ë„: {disaster_analysis.get('severity')}
        - ì˜í–¥ ë°˜ê²½: {disaster_analysis.get('affected_radius_km')} km
        
        ì—ë„ˆì§€ ë¶„ì„ ê²°ê³¼:
        - ì—ë„ˆì§€ ë°¸ëŸ°ìŠ¤: {energy_analysis.get('balance')} kW
        - ì‰ì—¬ ì§€ì—­: {len(energy_analysis.get('surplus_assets', []))}ê°œ
        - ë¶€ì¡± ì§€ì—­: {len(energy_analysis.get('deficit_assets', []))}ê°œ
        
        ì „ë ¥ë§ ë¶„ì„ ê²°ê³¼:
        - í™œì„± ì—°ê²°: {grid_analysis.get('active_connections')}ê°œ
        - ì°¨ë‹¨ëœ ê²½ë¡œ: {grid_analysis.get('blocked_routes')}ê°œ
        """
        
        prompt = f"""
        ë‹¹ì‹ ì€ ì¬ë‚œ ìƒí™©ì—ì„œ ì—ë„ˆì§€ ì¬ë¶„ë°°ë¥¼ ê²°ì •í•˜ëŠ” AI ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°ì…ë‹ˆë‹¤.
        
        {context}
        
        ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ P2P ì—ë„ˆì§€ ì¬ë¶„ë°° ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ìƒì„±í•˜ì„¸ìš”:
        
        1. ìš°ì„ ìˆœìœ„ ê²°ì • ê¸°ì¤€
        2. ì—ë„ˆì§€ ì´ë™ ê³„íš (ì¶œë°œì§€ -> ëª©ì ì§€, ì–‘)
        3. ì‹¤í–‰ ìˆœì„œ
        4. ì˜ˆìƒ íš¨ê³¼
        5. ë¦¬ìŠ¤í¬ ë° ëŒ€ì‘ ë°©ì•ˆ
        
        JSON í˜•ì‹ìœ¼ë¡œ êµ¬ì¡°í™”ëœ ì‹¤í–‰ ê³„íšì„ ì œê³µí•˜ì„¸ìš”.
        """
        
        decision = await self.llm.apredict(prompt)
        
        # ì‹¤í–‰ ê°€ëŠ¥í•œ ì•¡ì…˜ìœ¼ë¡œ ë³€í™˜
        actions = self._parse_decision_to_actions(
            decision,
            energy_analysis.get('surplus_assets', []),
            energy_analysis.get('deficit_assets', [])
        )
        
        result = {
            "agent": self.agent_name,
            "timestamp": "2024-01-15T10:30:00Z",
            "decision": decision,
            "actions": actions,
            "confidence_score": self._calculate_confidence(
                disaster_analysis, energy_analysis, grid_analysis
            ),
            "estimated_impact": {
                "energy_redistributed_kwh": sum(a.get("amount", 0) for a in actions),
                "affected_population": disaster_analysis.get("affected_population", 0),
                "recovery_time_hours": self._estimate_recovery_time(disaster_analysis)
            }
        }
        
        self.add_to_memory(result)
        logger.info(f"Decision made: {len(actions)} actions planned")
        
        return result
    
    def _parse_decision_to_actions(
        self,
        decision: str,
        surplus_assets: List[str],
        deficit_assets: List[str]
    ) -> List[Dict[str, Any]]:
        """ì˜ì‚¬ê²°ì •ì„ ì‹¤í–‰ ê°€ëŠ¥í•œ ì•¡ì…˜ìœ¼ë¡œ ë³€í™˜"""
        # ê°„ë‹¨í•œ ë§¤ì¹­ ì•Œê³ ë¦¬ì¦˜
        actions = []
        
        for i, (source, target) in enumerate(zip(surplus_assets, deficit_assets)):
            actions.append({
                "id": f"action_{i+1}",
                "type": "energy_transfer",
                "source_asset_id": source,
                "target_asset_id": target,
                "amount": 50.0,  # kW (ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ ê³„ì‚° í•„ìš”)
                "priority": i + 1,
                "estimated_duration_minutes": 30
            })
        
        return actions
    
    def _calculate_confidence(self, *analyses) -> float:
        """ì˜ì‚¬ê²°ì • ì‹ ë¢°ë„ ê³„ì‚°"""
        # ë‹¨ìˆœí™”ëœ ê³„ì‚° (ì‹¤ì œë¡œëŠ” ë” ë³µì¡í•œ ë¡œì§ í•„ìš”)
        return 0.85
    
    def _estimate_recovery_time(self, disaster_analysis: Dict) -> float:
        """ë³µêµ¬ ì‹œê°„ ì¶”ì •"""
        severity = disaster_analysis.get("severity", "moderate")
        time_map = {
            "minor": 2,
            "moderate": 6,
            "severe": 24,
            "critical": 72
        }
        return time_map.get(severity, 12)
```

### 5. Multi-Agent Orchestrator

```python
# backend/src/agents/orchestrator.py
from src.agents.disaster_analyzer import DisasterAnalyzerAgent
from src.agents.energy_analyzer import EnergyAnalyzerAgent
from src.agents.decision_maker import DecisionMakerAgent
from typing import Dict, Any
import asyncio
import logging

logger = logging.getLogger(__name__)

class MultiAgentOrchestrator:
    """ë©€í‹° ì—ì´ì „íŠ¸ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°"""
    
    def __init__(self):
        self.disaster_agent = DisasterAnalyzerAgent()
        self.energy_agent = EnergyAnalyzerAgent()
        self.decision_agent = DecisionMakerAgent()
        logger.info("Multi-agent orchestrator initialized")
    
    async def orchestrate(
        self,
        disaster_data: Dict[str, Any],
        energy_data: Dict[str, Any],
        grid_data: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ì „ì²´ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ ì‹¤í–‰"""
        
        logger.info("Starting orchestration...")
        
        # ë³‘ë ¬ ë¶„ì„ ì‹¤í–‰
        disaster_analysis, energy_analysis = await asyncio.gather(
            self.disaster_agent.analyze(disaster_data),
            self.energy_agent.analyze(energy_data)
        )
        
        # Grid ë¶„ì„ (ê°„ë‹¨í•œ ì˜ˆì‹œ)
        grid_analysis = {
            "active_connections": grid_data.get("connections", 0),
            "blocked_routes": grid_data.get("failures", 0)
        }
        
        # ìµœì¢… ì˜ì‚¬ê²°ì •
        decision = await self.decision_agent.make_decision(
            disaster_analysis,
            energy_analysis,
            grid_analysis
        )
        
        result = {
            "orchestration_id": self._generate_id(),
            "timestamp": "2024-01-15T10:30:00Z",
            "disaster_analysis": disaster_analysis,
            "energy_analysis": energy_analysis,
            "grid_analysis": grid_analysis,
            "final_decision": decision,
            "status": "completed"
        }
        
        logger.info(f"Orchestration completed: {len(decision['actions'])} actions")
        
        return result
    
    def _generate_id(self) -> str:
        """ê³ ìœ  ID ìƒì„±"""
        import uuid
        return str(uuid.uuid4())
```

---

## ğŸ§  GNN ê¸°ë°˜ ì—ë„ˆì§€ ì˜ˆì¸¡

### GNN ëª¨ë¸ ì •ì˜

```python
# backend/src/ml/gnn_model.py
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn import GCNConv, GATConv
from torch_geometric.data import Data
import logging

logger = logging.getLogger(__name__)

class EnergyGNN(nn.Module):
    """ì—ë„ˆì§€ ì˜ˆì¸¡ì„ ìœ„í•œ Graph Neural Network"""
    
    def __init__(
        self,
        num_node_features: int,
        hidden_channels: int = 64,
        num_layers: int = 3,
        dropout: float = 0.2
    ):
        super(EnergyGNN, self).__init__()
        
        self.num_layers = num_layers
        self.dropout = dropout
        
        # GNN ë ˆì´ì–´
        self.convs = nn.ModuleList()
        self.convs.append(GCNConv(num_node_features, hidden_channels))
        
        for _ in range(num_layers - 2):
            self.convs.append(GCNConv(hidden_channels, hidden_channels))
        
        self.convs.append(GCNConv(hidden_channels, hidden_channels))
        
        # ì˜ˆì¸¡ ë ˆì´ì–´
        self.predictor = nn.Sequential(
            nn.Linear(hidden_channels, hidden_channels // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_channels // 2, 1)  # ì—ë„ˆì§€ ìƒì‚°/ì†Œë¹„ ì˜ˆì¸¡
        )
        
        logger.info(f"GNN model initialized with {num_layers} layers")
    
    def forward(self, data: Data) -> torch.Tensor:
        """ìˆœì „íŒŒ"""
        x, edge_index = data.x, data.edge_index
        
        # GNN ë ˆì´ì–´ í†µê³¼
        for i, conv in enumerate(self.convs):
            x = conv(x, edge_index)
            if i < len(self.convs) - 1:
                x = F.relu(x)
                x = F.dropout(x, p=self.dropout, training=self.training)
        
        # ì˜ˆì¸¡
        out = self.predictor(x)
        
        return out

class EnergyGraphBuilder:
    """ì—ë„ˆì§€ ë„¤íŠ¸ì›Œí¬ë¥¼ ê·¸ë˜í”„ë¡œ ë³€í™˜"""
    
    @staticmethod
    def build_graph(
        nodes: list,  # ë…¸ë“œ (ì—ë„ˆì§€ ìì‚°) ì •ë³´
        edges: list,  # ì—£ì§€ (ì—°ê²°) ì •ë³´
        node_features: dict  # ë…¸ë“œë³„ íŠ¹ì§•
    ) -> Data:
        """ê·¸ë˜í”„ ë°ì´í„° êµ¬ì¶•"""
        
        # ë…¸ë“œ íŠ¹ì§• í…ì„œ ìƒì„±
        x = torch.tensor([
            [
                node_features[node['id']]['production'],
                node_features[node['id']]['consumption'],
                node_features[node['id']]['capacity'],
                node_features[node['id']]['battery_soc'],
                node_features[node['id']]['weather_score']
            ]
            for node in nodes
        ], dtype=torch.float)
        
        # ì—£ì§€ ì¸ë±ìŠ¤ í…ì„œ ìƒì„±
        edge_index = torch.tensor([
            [edge['source_idx'] for edge in edges],
            [edge['target_idx'] for edge in edges]
        ], dtype=torch.long)
        
        # ê·¸ë˜í”„ ë°ì´í„° ê°ì²´ ìƒì„±
        data = Data(x=x, edge_index=edge_index)
        
        logger.info(f"Graph built: {data.num_nodes} nodes, {data.num_edges} edges")
        
        return data
```

### ì˜ˆì¸¡ ì„œë¹„ìŠ¤

```python
# backend/src/services/prediction_service.py
from src.ml.gnn_model import EnergyGNN, EnergyGraphBuilder
import torch
import logging
from typing import Dict, List, Any

logger = logging.getLogger(__name__)

class PredictionService:
    """ì—ë„ˆì§€ ì˜ˆì¸¡ ì„œë¹„ìŠ¤"""
    
    def __init__(self, model_path: str = None):
        self.model = EnergyGNN(num_node_features=5, hidden_channels=64)
        
        if model_path:
            self.model.load_state_dict(torch.load(model_path))
            logger.info(f"Model loaded from {model_path}")
        
        self.model.eval()
        self.graph_builder = EnergyGraphBuilder()
    
    async def predict_energy_flow(
        self,
        assets: List[Dict[str, Any]],
        connections: List[Dict[str, Any]],
        current_readings: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ì—ë„ˆì§€ íë¦„ ì˜ˆì¸¡"""
        
        # ë…¸ë“œ íŠ¹ì§• ì¤€ë¹„
        node_features = {}
        nodes = []
        
        for i, asset in enumerate(assets):
            asset_id = asset['id']
            nodes.append({'id': asset_id, 'idx': i})
            
            node_features[asset_id] = {
                'production': current_readings.get(f'{asset_id}_production', 0),
                'consumption': current_readings.get(f'{asset_id}_consumption', 0),
                'capacity': asset.get('capacity_kw', 0),
                'battery_soc': current_readings.get(f'{asset_id}_soc', 0.5),
                'weather_score': current_readings.get('weather_score', 0.7)
            }
        
        # ì—£ì§€ ì¤€ë¹„
        edges = [
            {
                'source_idx': next(n['idx'] for n in nodes if n['id'] == conn['source_id']),
                'target_idx': next(n['idx'] for n in nodes if n['id'] == conn['target_id'])
            }
            for conn in connections
        ]
        
        # ê·¸ë˜í”„ êµ¬ì¶•
        graph_data = self.graph_builder.build_graph(nodes, edges, node_features)
        
        # ì˜ˆì¸¡ ì‹¤í–‰
        with torch.no_grad():
            predictions = self.model(graph_data)
        
        # ê²°ê³¼ ì •ë¦¬
        results = {
            'predictions': [
                {
                    'asset_id': nodes[i]['id'],
                    'predicted_net_energy': float(predictions[i]),
                    'current_production': node_features[nodes[i]['id']]['production'],
                    'current_consumption': node_features[nodes[i]['id']]['consumption']
                }
                for i in range(len(nodes))
            ],
            'timestamp': '2024-01-15T10:30:00Z',
            'model_confidence': 0.85
        }
        
        logger.info(f"Prediction completed for {len(nodes)} assets")
        
        return results
```

---

## ğŸ”— ì˜¨í†¨ë¡œì§€ ì„œë¹„ìŠ¤

### SPARQL ì¿¼ë¦¬ ì„œë¹„ìŠ¤

```python
# backend/src/services/ontology_service.py
from SPARQLWrapper import SPARQLWrapper, JSON
from typing import List, Dict, Any
import logging

logger = logging.getLogger(__name__)

class OntologyService:
    """ì˜¨í†¨ë¡œì§€ ì¿¼ë¦¬ ì„œë¹„ìŠ¤"""
    
    def __init__(self, fuseki_url: str = "http://localhost:3030"):
        self.fuseki_url = fuseki_url
        self.sparql = SPARQLWrapper(f"{fuseki_url}/energy_ontology/query")
        self.sparql.setReturnFormat(JSON)
        logger.info(f"Ontology service initialized: {fuseki_url}")
    
    async def query_affected_assets(
        self,
        disaster_location: Dict[str, float],
        radius_km: float
    ) -> List[Dict[str, Any]]:
        """ì¬ë‚œ ì˜í–¥ê¶Œ ë‚´ ì—ë„ˆì§€ ìì‚° ì¡°íšŒ"""
        
        lat = disaster_location['lat']
        lon = disaster_location['lon']
        
        query = f"""
        PREFIX geo: <http://www.opengis.net/ont/geosparql#>
        PREFIX energy: <http://example.org/ontology/energy#>
        PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>
        
        SELECT ?asset ?name ?type ?location
        WHERE {{
            ?asset a energy:EnergyAsset ;
                   rdfs:label ?name ;
                   energy:assetType ?type ;
                   geo:hasGeometry ?geom .
            
            ?geom geo:asWKT ?location .
            
            FILTER(
                geof:distance(
                    ?location,
                    "POINT({lon} {lat})"^^geo:wktLiteral,
                    uom:kilometre
                ) < {radius_km}
            )
        }}
        """
        
        self.sparql.setQuery(query)
        results = self.sparql.query().convert()
        
        affected_assets = [
            {
                'uri': result['asset']['value'],
                'name': result['name']['value'],
                'type': result['type']['value'],
                'location': result['location']['value']
            }
            for result in results['results']['bindings']
        ]
        
        logger.info(f"Found {len(affected_assets)} affected assets")
        
        return affected_assets
    
    async def infer_energy_relationships(
        self,
        asset_id: str
    ) -> Dict[str, Any]:
        """ì—ë„ˆì§€ ìì‚° ê°„ ê´€ê³„ ì¶”ë¡ """
        
        query = f"""
        PREFIX energy: <http://example.org/ontology/energy#>
        PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>
        
        SELECT ?relatedAsset ?relationType ?name
        WHERE {{
            <{asset_id}> energy:connectedTo ?relatedAsset .
            ?relatedAsset rdfs:label ?name ;
                         energy:connectionType ?relationType .
        }}
        """
        
        self.sparql.setQuery(query)
        results = self.sparql.query().convert()
        
        relationships = {
            'asset_id': asset_id,
            'connections': [
                {
                    'asset': result['relatedAsset']['value'],
                    'type': result['relationType']['value'],
                    'name': result['name']['value']
                }
                for result in results['results']['bindings']
            ]
        }
        
        return relationships
```

### ì˜¨í†¨ë¡œì§€ ìŠ¤í‚¤ë§ˆ ì˜ˆì‹œ (Turtle í˜•ì‹)

```turtle
# ontology/schemas/energy_ontology.ttl

@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .
@prefix owl: <http://www.w3.org/2002/07/owl#> .
@prefix energy: <http://example.org/ontology/energy#> .
@prefix geo: <http://www.opengis.net/ont/geosparql#> .

# í´ë˜ìŠ¤ ì •ì˜
energy:EnergyAsset a owl:Class ;
    rdfs:label "Energy Asset" ;
    rdfs:comment "An energy generation or consumption facility" .

energy:SolarPanel a owl:Class ;
    rdfs:subClassOf energy:EnergyAsset ;
    rdfs:label "Solar Panel" .

energy:WindTurbine a owl:Class ;
    rdfs:subClassOf energy:EnergyAsset ;
    rdfs:label "Wind Turbine" .

energy:BatteryStorage a owl:Class ;
    rdfs:subClassOf energy:EnergyAsset ;
    rdfs:label "Battery Storage" .

# ì†ì„± ì •ì˜
energy:hasCapacity a owl:DatatypeProperty ;
    rdfs:domain energy:EnergyAsset ;
    rdfs:range xsd:float ;
    rdfs:label "has capacity (kW)" .

energy:connectedTo a owl:ObjectProperty ;
    rdfs:domain energy:EnergyAsset ;
    rdfs:range energy:EnergyAsset ;
    rdfs:label "connected to" .

energy:producesEnergy a owl:DatatypeProperty ;
    rdfs:domain energy:EnergyAsset ;
    rdfs:range xsd:float ;
    rdfs:label "produces energy (kW)" .

# ì¸ìŠ¤í„´ìŠ¤ ì˜ˆì‹œ
energy:asset_solar_001 a energy:SolarPanel ;
    rdfs:label "Pyeongtaek Solar Farm 1" ;
    energy:hasCapacity "500.0"^^xsd:float ;
    energy:connectedTo energy:asset_battery_001 ;
    geo:hasGeometry [
        a geo:Geometry ;
        geo:asWKT "POINT(126.7958 36.9925)"^^geo:wktLiteral
    ] .
```

---

## ğŸ“¡ IoT ë°ì´í„° ìˆ˜ì§‘

### MQTT Handler

```python
# backend/src/utils/mqtt_client.py
import paho.mqtt.client as mqtt
from typing import Callable, Dict, Any
import json
import logging

logger = logging.getLogger(__name__)

class MQTTHandler:
    """MQTT ë°ì´í„° ìˆ˜ì§‘ í•¸ë“¤ëŸ¬"""
    
    def __init__(
        self,
        broker_host: str,
        broker_port: int = 1883,
        username: str = None,
        password: str = None
    ):
        self.broker_host = broker_host
        self.broker_port = broker_port
        self.client = mqtt.Client()
        
        if username and password:
            self.client.username_pw_set(username, password)
        
        self.client.on_connect = self._on_connect
        self.client.on_message = self._on_message
        self.client.on_disconnect = self._on_disconnect
        
        self.message_callbacks: Dict[str, Callable] = {}
        
        logger.info(f"MQTT handler initialized: {broker_host}:{broker_port}")
    
    def _on_connect(self, client, userdata, flags, rc):
        """ì—°ê²° ì½œë°±"""
        if rc == 0:
            logger.info("Connected to MQTT broker")
            # ì¬ì—°ê²° ì‹œ ìë™ êµ¬ë…
            for topic in self.message_callbacks.keys():
                client.subscribe(topic)
                logger.info(f"Subscribed to topic: {topic}")
        else:
            logger.error(f"Connection failed with code {rc}")
    
    def _on_message(self, client, userdata, msg):
        """ë©”ì‹œì§€ ìˆ˜ì‹  ì½œë°±"""
        try:
            payload = json.loads(msg.payload.decode())
            topic = msg.topic
            
            logger.debug(f"Received message on topic {topic}")
            
            # ë“±ë¡ëœ ì½œë°± ì‹¤í–‰
            if topic in self.message_callbacks:
                self.message_callbacks[topic](topic, payload)
            
            # ì™€ì¼ë“œì¹´ë“œ ë§¤ì¹­
            for registered_topic, callback in self.message_callbacks.items():
                if self._topic_matches(registered_topic, topic):
                    callback(topic, payload)
        
        except Exception as e:
            logger.error(f"Error processing message: {e}")
    
    def _on_disconnect(self, client, userdata, rc):
        """ì—°ê²° í•´ì œ ì½œë°±"""
        logger.warning(f"Disconnected from MQTT broker with code {rc}")
    
    def connect(self):
        """ë¸Œë¡œì»¤ ì—°ê²°"""
        self.client.connect(self.broker_host, self.broker_port, 60)
        self.client.loop_start()
    
    def disconnect(self):
        """ë¸Œë¡œì»¤ ì—°ê²° í•´ì œ"""
        self.client.loop_stop()
        self.client.disconnect()
    
    def subscribe(self, topic: str, callback: Callable[[str, Dict], None]):
        """í† í”½ êµ¬ë…"""
        self.message_callbacks[topic] = callback
        self.client.subscribe(topic)
        logger.info(f"Subscribed to topic: {topic}")
    
    def publish(self, topic: str, payload: Dict[str, Any]):
        """ë©”ì‹œì§€ ë°œí–‰"""
        self.client.publish(topic, json.dumps(payload))
    
    @staticmethod
    def _topic_matches(pattern: str, topic: str) -> bool:
        """í† í”½ íŒ¨í„´ ë§¤ì¹­ (+: single level, #: multi level)"""
        pattern_parts = pattern.split('/')
        topic_parts = topic.split('/')
        
        if len(pattern_parts) != len(topic_parts) and '#' not in pattern:
            return False
        
        for p, t in zip(pattern_parts, topic_parts):
            if p == '#':
                return True
            if p != '+' and p != t:
                return False
        
        return True
```

### Kafka Producer

```python
# backend/src/utils/kafka_producer.py
from kafka import KafkaProducer
import json
import logging
from typing import Dict, Any

logger = logging.getLogger(__name__)

class EnergyKafkaProducer:
    """Kafka í”„ë¡œë“€ì„œ"""
    
    def __init__(self, bootstrap_servers: str):
        self.producer = KafkaProducer(
            bootstrap_servers=bootstrap_servers,
            value_serializer=lambda v: json.dumps(v).encode('utf-8'),
            key_serializer=lambda k: k.encode('utf-8') if k else None
        )
        logger.info(f"Kafka producer initialized: {bootstrap_servers}")
    
    def send_iot_data(self, device_id: str, data: Dict[str, Any]):
        """IoT ë°ì´í„° ì „ì†¡"""
        topic = "iot-data"
        
        message = {
            "device_id": device_id,
            "timestamp": data.get("timestamp"),
            "metrics": data.get("metrics", {}),
            "metadata": data.get("metadata", {})
        }
        
        future = self.producer.send(topic, key=device_id, value=message)
        
        try:
            record_metadata = future.get(timeout=10)
            logger.debug(f"Message sent to {record_metadata.topic} partition {record_metadata.partition}")
        except Exception as e:
            logger.error(f"Failed to send message: {e}")
    
    def close(self):
        """í”„ë¡œë“€ì„œ ì¢…ë£Œ"""
        self.producer.close()
```

---

## ğŸ“Š ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§

### WebSocket Handler

```python
# backend/src/api/v1/websocket.py
from fastapi import WebSocket, WebSocketDisconnect
from typing import Dict, Set
import json
import logging

logger = logging.getLogger(__name__)

class ConnectionManager:
    """WebSocket ì—°ê²° ê´€ë¦¬ì"""
    
    def __init__(self):
        self.active_connections: Dict[str, Set[WebSocket]] = {}
    
    async def connect(self, websocket: WebSocket, channel: str):
        """ì—°ê²° ìˆ˜ë½"""
        await websocket.accept()
        
        if channel not in self.active_connections:
            self.active_connections[channel] = set()
        
        self.active_connections[channel].add(websocket)
        logger.info(f"Client connected to channel: {channel}")
    
    def disconnect(self, websocket: WebSocket, channel: str):
        """ì—°ê²° í•´ì œ"""
        if channel in self.active_connections:
            self.active_connections[channel].discard(websocket)
        logger.info(f"Client disconnected from channel: {channel}")
    
    async def broadcast(self, channel: str, message: dict):
        """ì±„ë„ì— ë©”ì‹œì§€ ë¸Œë¡œë“œìºìŠ¤íŠ¸"""
        if channel not in self.active_connections:
            return
        
        disconnected = set()
        
        for connection in self.active_connections[channel]:
            try:
                await connection.send_json(message)
            except WebSocketDisconnect:
                disconnected.add(connection)
            except Exception as e:
                logger.error(f"Error broadcasting message: {e}")
                disconnected.add(connection)
        
        # ì—°ê²° í•´ì œëœ í´ë¼ì´ì–¸íŠ¸ ì œê±°
        self.active_connections[channel] -= disconnected

manager = ConnectionManager()

# FastAPI ë¼ìš°í„°ì—ì„œ ì‚¬ìš©
from fastapi import APIRouter

ws_router = APIRouter()

@ws_router.websocket("/ws/{channel}")
async def websocket_endpoint(websocket: WebSocket, channel: str):
    """WebSocket ì—”ë“œí¬ì¸íŠ¸"""
    await manager.connect(websocket, channel)
    
    try:
        while True:
            # í´ë¼ì´ì–¸íŠ¸ë¡œë¶€í„° ë©”ì‹œì§€ ìˆ˜ì‹  (í•„ìš”ì‹œ)
            data = await websocket.receive_text()
            # ì²˜ë¦¬ ë¡œì§...
    
    except WebSocketDisconnect:
        manager.disconnect(websocket, channel)
```

---

ì´ ì½”ë“œ ìƒ˜í”Œë“¤ì„ ê¸°ë°˜ìœ¼ë¡œ í•µì‹¬ ê¸°ëŠ¥ì„ êµ¬í˜„í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤!
ë” ìƒì„¸í•œ êµ¬í˜„ì´ë‚˜ ë‹¤ë¥¸ ê¸°ëŠ¥ì˜ ì½”ë“œê°€ í•„ìš”í•˜ì‹œë©´ ë§ì”€í•´ì£¼ì„¸ìš”.
